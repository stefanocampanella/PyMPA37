#! /usr/bin/env python
import argparse
import logging
import os
import sys
from pathlib import Path
from datetime import datetime

from yaml import load, FullLoader
from tqdm import tqdm
import pandas as pd

from pympa.pympa import read_templates, range_days, read_continuous_stream, correlation_detector

parser = argparse.ArgumentParser(prog="PyMPA37",
                                 description="A software package for phase match filtering")
parser.add_argument("root", help="Working directory", type=Path, nargs='?', default=os.getcwd())
parser.add_argument("date", help="Datetime begin", type=datetime.fromisoformat)
parser.add_argument("output", help="Output file path")
parser.add_argument("--min_channels", help="Minimum number of channels", type=int, default=6)
parser.add_argument("--max_channels", help="Maximum number of channels", type=int, default=15)
parser.add_argument("--lowpass_freq", help="Lowpass filter frequency", type=float, default=3.0)
parser.add_argument("--highpass_freq", help="Highpass filter frequency", type=float, default=8.0)
parser.add_argument("--travel_times", help="Relative path to travel times directory", type=Path, default='travel_times')
parser.add_argument("--templates", help="Relative path to templates directory", type=Path, default='templates')
parser.add_argument("--catalog", help="Relative path to event catalog", type=Path, default='templates.zmap')
parser.add_argument("--continuous", help="Relative path to continuous traces directory", type=Path, default='24h')
cli_args = parser.parse_args()

logging.basicConfig(stream=sys.stderr,
                    format='%(levelname)s: %(message)s',
                    level=logging.ERROR)

if __name__ == '__main__':
    is_interactive = os.isatty(sys.stdout.fileno())

    with open("settings.yaml") as settings_file:
        settings = load(settings_file, FullLoader)

    templates = read_templates(cli_args.root / cli_args.templates,
                               cli_args.root / cli_args.travel_times,
                               cli_args.root / cli_args.catalog,
                               cli_args.min_channels,
                               num_templates_bounds=settings['templates']['range'],
                               num_tt_channels_bounds=settings['travel_times']['num_channels_bounds'])

    events = []
    day_stream = read_continuous_stream(cli_args.continuous,
                                        cli_args.date,
                                        cli_args.min_channels,
                                        freqmin=cli_args.lowpass_freq,
                                        freqmax=cli_args.highpass_freq)
    for template_number, template_stream, travel_times, template_magnitude in tqdm(templates, disable=not is_interactive):
        try:
            detections = correlation_detector(template_stream,
                                              day_stream,
                                              travel_times,
                                              template_magnitude,
                                              settings['detector']['threshold_factor'],
                                              settings['detector']['samples_tolerance'],
                                              correlations_std_bounds=settings['detector']['correlations_std_bounds'])
            for detection in detections:
                date, magnitude, correlation, stack_height, stack_dmad, channels = detection
                num_channels = sum(1 for _, cc, _ in channels if cc > settings['cc_filter']['threshold'])
                if num_channels >= settings['cc_filter']['num_channels']:
                    record = (template_number, date.datetime, magnitude, correlation,
                              stack_height, stack_dmad, num_channels)
                    events.append(record)
        except ValueError as err:
            logging.info(f"{err} occurred during processing of template {template_number}, {date}")


    events_dataframe = pd.DataFrame.from_records(events,
                                                 columns=['template', 'date', 'magnitude',
                                                          'correlation', 'stack_height', 'stack_dmad',
                                                          'num_channels'])
    events_dataframe.sort_values(by=['template', 'date'], inplace=True)
    events_dataframe['crt_pre'] = events_dataframe['stack_height'] / events_dataframe['stack_dmad']
    events_dataframe['crt_post'] = events_dataframe['correlation'] / events_dataframe['stack_dmad']
    logging.info(f"Writing outputs to {cli_args.output}")
    events_dataframe.to_csv(cli_args.output,
                            index=False,
                            date_format='%Y-%m-%dT%H:%M:%S.%fZ',
                            float_format='%.3f',
                            columns=['template', 'date', 'magnitude',
                                     'correlation', 'crt_post',
                                     'stack_height', 'crt_pre',
                                     'num_channels'],
                            header=False, sep=' ')
